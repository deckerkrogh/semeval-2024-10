{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/deckerkrogh/semeval-2024-10/blob/main/DualLSTMv2_semeval.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install unzip\n",
        "!wget https://nlp.stanford.edu/data/glove.6B.zip\n",
        "!unzip /content/glove.6B.zip"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3EnkBB17qRfr",
        "outputId": "2437eff6-700b-42df-97f8-06744715e406"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting unzip\n",
            "  Downloading unzip-1.0.0.tar.gz (704 bytes)\n",
            "  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Building wheels for collected packages: unzip\n",
            "  Building wheel for unzip (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for unzip: filename=unzip-1.0.0-py3-none-any.whl size=1280 sha256=79abb825a59394f4a5a95c3b2b98197cfddf1d25a9dea8b1ed6bd934ed39feba\n",
            "  Stored in directory: /root/.cache/pip/wheels/80/dc/7a/f8af45bc239e7933509183f038ea8d46f3610aab82b35369f4\n",
            "Successfully built unzip\n",
            "Installing collected packages: unzip\n",
            "Successfully installed unzip-1.0.0\n",
            "--2023-12-07 02:48:58--  https://nlp.stanford.edu/data/glove.6B.zip\n",
            "Resolving nlp.stanford.edu (nlp.stanford.edu)... 171.64.67.140\n",
            "Connecting to nlp.stanford.edu (nlp.stanford.edu)|171.64.67.140|:443... connected.\n",
            "HTTP request sent, awaiting response... 301 Moved Permanently\n",
            "Location: https://downloads.cs.stanford.edu/nlp/data/glove.6B.zip [following]\n",
            "--2023-12-07 02:48:58--  https://downloads.cs.stanford.edu/nlp/data/glove.6B.zip\n",
            "Resolving downloads.cs.stanford.edu (downloads.cs.stanford.edu)... 171.64.64.22\n",
            "Connecting to downloads.cs.stanford.edu (downloads.cs.stanford.edu)|171.64.64.22|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 862182613 (822M) [application/zip]\n",
            "Saving to: ‘glove.6B.zip’\n",
            "\n",
            "glove.6B.zip        100%[===================>] 822.24M  5.03MB/s    in 2m 39s  \n",
            "\n",
            "2023-12-07 02:51:37 (5.17 MB/s) - ‘glove.6B.zip’ saved [862182613/862182613]\n",
            "\n",
            "Archive:  /content/glove.6B.zip\n",
            "  inflating: glove.6B.50d.txt        \n",
            "  inflating: glove.6B.100d.txt       \n",
            "  inflating: glove.6B.200d.txt       \n",
            "  inflating: glove.6B.300d.txt       \n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "\n",
        "import torch\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "import torch.nn as nn\n",
        "\n",
        "import gensim\n",
        "from gensim.test.utils import datapath, get_tmpfile\n",
        "from gensim.scripts.glove2word2vec import glove2word2vec\n",
        "\n",
        "from sklearn.metrics import accuracy_score\n"
      ],
      "metadata": {
        "id": "dYFNadU0U2fP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# TODO: use wget\n",
        "glovefile = 'glove.6B.100d.txt'\n",
        "glove_file_datapath = datapath(glovefile)\n",
        "tmp_file = get_tmpfile('word2vec.txt')\n",
        "\n",
        "_ = glove2word2vec(glovefile, tmp_file)\n",
        "word2vec_weights = gensim.models.KeyedVectors.load_word2vec_format(tmp_file)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ZqDnegCdj3Vc",
        "outputId": "63cba408-c47f-4605-b7ba-a5faa9832a8f"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<ipython-input-3-431b93b6bfb0>:6: DeprecationWarning: Call to deprecated `glove2word2vec` (KeyedVectors.load_word2vec_format(.., binary=False, no_header=True) loads GLoVE text vectors.).\n",
            "  _ = glove2word2vec(glovefile, tmp_file)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(word2vec_weights['cat'])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "HcLrLsdN85FQ",
        "outputId": "dd5b22b8-ac07-4033-a2f2-9857f3fd8eca"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[ 0.23088    0.28283    0.6318    -0.59411   -0.58599    0.63255\n",
            "  0.24402   -0.14108    0.060815  -0.7898    -0.29102    0.14287\n",
            "  0.72274    0.20428    0.1407     0.98757    0.52533    0.097456\n",
            "  0.8822     0.51221    0.40204    0.21169   -0.013109  -0.71616\n",
            "  0.55387    1.1452    -0.88044   -0.50216   -0.22814    0.023885\n",
            "  0.1072     0.083739   0.55015    0.58479    0.75816    0.45706\n",
            " -0.28001    0.25225    0.68965   -0.60972    0.19578    0.044209\n",
            " -0.31136   -0.68826   -0.22721    0.46185   -0.77162    0.10208\n",
            "  0.55636    0.067417  -0.57207    0.23735    0.4717     0.82765\n",
            " -0.29263   -1.3422    -0.099277   0.28139    0.41604    0.10583\n",
            "  0.62203    0.89496   -0.23446    0.51349    0.99379    1.1846\n",
            " -0.16364    0.20653    0.73854    0.24059   -0.96473    0.13481\n",
            " -0.0072484  0.33016   -0.12365    0.27191   -0.40951    0.021909\n",
            " -0.6069     0.40755    0.19566   -0.41802    0.18636   -0.032652\n",
            " -0.78571   -0.13847    0.044007  -0.084423   0.04911    0.24104\n",
            "  0.45273   -0.18682    0.46182    0.089068  -0.18185   -0.01523\n",
            " -0.7368    -0.14532    0.15104   -0.71493  ]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#@title Load data\n",
        "\n",
        "train_url = 'https://raw.githubusercontent.com/deckerkrogh/nlp243_data/main/datasets/task3_train.json'\n",
        "train_df = pd.read_json(train_url)"
      ],
      "metadata": {
        "id": "Byr9MTUrt7Bl"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#@title Sequencer\n",
        "\n",
        "class PreTrainedSequencer(object):\n",
        "    # Maps text to index and then to corresponding embedding\n",
        "    # Source: Jesh\n",
        "    def __init__(self, corpus, gensim_w2v, embedding_dim, bos_token='<s>', eos_token='</s>', unk_token='<unk>', pad_token='<pad>'):\n",
        "\n",
        "        print(corpus)\n",
        "        self.idx2word = {}\n",
        "        self.word2idx = {}\n",
        "        self.embedding_dim = embedding_dim\n",
        "        self.w2v = gensim_w2v\n",
        "\n",
        "        self.w2v.add_vectors([bos_token], [np.random.uniform(low=-1, high=1.0, size=(self.embedding_dim,))])\n",
        "        self.w2v.add_vectors([eos_token], [np.random.uniform(low=-1, high=1.0, size=(self.embedding_dim,))])\n",
        "        self.w2v.add_vectors([unk_token], [np.random.uniform(low=-1, high=1.0, size=(self.embedding_dim,))])\n",
        "        self.w2v.add_vectors([pad_token], [np.random.uniform(low=-1, high=1.0, size=(self.embedding_dim,))])\n",
        "\n",
        "        self.bos_index = self.add_token(bos_token)\n",
        "        self.eos_index = self.add_token(eos_token)\n",
        "        self.unk_index = self.add_token(unk_token)\n",
        "        self.pad_index = self.add_token(pad_token)\n",
        "        self.tokenizer = lambda text: [t for t in text.split(' ')]\n",
        "\n",
        "        for _sentence in corpus:\n",
        "            #print(_sentence)\n",
        "            for _token in self.tokenizer(_sentence):\n",
        "                self.add_token(_token)\n",
        "        self.pre_trained_embeddings = torch.zeros([len(self.idx2word.keys()), self.embedding_dim])\n",
        "\n",
        "        for idx, word in self.idx2word.items():\n",
        "            if self.w2v.has_index_for(word):\n",
        "                self.pre_trained_embeddings[idx] = torch.tensor(self.w2v.get_vector(self.w2v.key_to_index.get(word), norm=True))\n",
        "            else:\n",
        "                self.pre_trained_embeddings[idx] = torch.tensor(self.w2v.get_vector(self.w2v.key_to_index.get(unk_token), norm=True))\n",
        "\n",
        "    def add_token(self, token):\n",
        "        if token not in self.word2idx:\n",
        "            self.word2idx[token] = new_index = len(self.word2idx)\n",
        "            self.idx2word[new_index] = token\n",
        "            return new_index\n",
        "        else:\n",
        "            return self.word2idx[token]\n",
        "\n",
        "    def encode(self, text):\n",
        "        tokens = self.tokenizer(text)\n",
        "        sequence = []\n",
        "        sequence.append(self.bos_index)\n",
        "\n",
        "        for token in tokens:\n",
        "\n",
        "            index = self.word2idx.get(token, self.unk_index)\n",
        "            sequence.append(index)\n",
        "\n",
        "        sequence.append(self.eos_index)\n",
        "        return sequence\n",
        "\n",
        "    def create_padded_tensor(self, sequences):\n",
        "        lengths = [len(sequence) for sequence in sequences]\n",
        "        max_seq_len = max(lengths)\n",
        "        tensor = torch.full((len(sequences), max_seq_len), self.pad_index, dtype=torch.long)\n",
        "\n",
        "        for i, sequence in enumerate(sequences):\n",
        "            for j, token in enumerate(sequence):\n",
        "                tensor[i][j] = token\n",
        "\n",
        "        return tensor, lengths"
      ],
      "metadata": {
        "id": "Xfy7j5bF869G"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#@title Dataset class\n",
        "\n",
        "class MELDDataset(Dataset):\n",
        "    def __init__(self, df, sequencer):\n",
        "        self.sequencer = sequencer\n",
        "\n",
        "        # Create list of (target, candidate, trigger) groupings\n",
        "        self.utt_pairs = []\n",
        "        for i, conversation in df.iterrows():\n",
        "            target = conversation['utterances'][-1]\n",
        "            for utt, trigger, in zip(conversation['utterances'], conversation['triggers']):\n",
        "                self.utt_pairs.append([target, utt, trigger])\n",
        "\n",
        "    def __getitem__(self, index):\n",
        "        # Return encode target, candidate, and trigger\n",
        "        utt_pair = self.utt_pairs[index]\n",
        "        x_target = self.sequencer.encode(utt_pair[0])\n",
        "        x_candidate = self.sequencer.encode(utt_pair[1])\n",
        "        y = utt_pair[2]\n",
        "        return x_target, x_candidate, y\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.utt_pairs)"
      ],
      "metadata": {
        "id": "Tc5PEV4VafTB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#@title Model\n",
        "\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "\n",
        "class DualLSTM(nn.Module):\n",
        "    def __init__(self, pad_index, embedding_dim, batch_size,\n",
        "                 pre_trained_embeddings, num_layers=1, hidden_size=100,\n",
        "                 dropout_p=0.1):\n",
        "        super(DualLSTM, self).__init__()\n",
        "\n",
        "        self.pad_index = pad_index\n",
        "        self.hidden_size = hidden_size\n",
        "        self.dropout_p = dropout_p\n",
        "        self.embedding_dim = embedding_dim\n",
        "        self.batch_size = batch_size\n",
        "        self.pre_trained_embeddings = pre_trained_embeddings\n",
        "\n",
        "        # Layers\n",
        "        self.embedding = nn.Embedding.from_pretrained(pre_trained_embeddings)\n",
        "        self.dropout = nn.Dropout(dropout_p)  # Added dropout after embedding\n",
        "\n",
        "        self.target_lstm = nn.LSTM(\n",
        "            input_size=embedding_dim,\n",
        "            hidden_size=hidden_size,\n",
        "            num_layers=num_layers,\n",
        "            dropout=dropout_p if num_layers > 1 else 0,\n",
        "            bidirectional=False,\n",
        "            batch_first=True\n",
        "        )\n",
        "\n",
        "        self.cand_lstm = nn.LSTM(\n",
        "            input_size=embedding_dim,\n",
        "            hidden_size=hidden_size,\n",
        "            num_layers=num_layers,\n",
        "            dropout=dropout_p if num_layers > 1 else 0,\n",
        "            bidirectional=False,\n",
        "            batch_first=True\n",
        "        )\n",
        "\n",
        "        # Concatenate the two lstm outputs\n",
        "        self.fc1 = nn.Linear(hidden_size * 2, hidden_size)\n",
        "        self.fc2 = nn.Linear(hidden_size, 10)\n",
        "        self.fc3 = nn.Linear(10, 1)\n",
        "\n",
        "    def forward(self, targets, target_l, candidates, cand_l):\n",
        "        embed_target = self.dropout(self.embedding(targets))  # Applying dropout\n",
        "        embed_cand = self.dropout(self.embedding(candidates))  # Applying dropout\n",
        "\n",
        "        packed_target_i = nn.utils.rnn.pack_padded_sequence(embed_target, target_l, batch_first=True, enforce_sorted=False)\n",
        "        packed_cand_i = nn.utils.rnn.pack_padded_sequence(embed_cand, cand_l, batch_first=True, enforce_sorted=False)\n",
        "\n",
        "        o, (target_h_n, h_c) = self.target_lstm(packed_target_i)\n",
        "        o, (cand_h_n, h_c) = self.cand_lstm(packed_cand_i)\n",
        "\n",
        "        target_h_n = target_h_n.squeeze()\n",
        "        cand_h_n = cand_h_n.squeeze()\n",
        "\n",
        "        # Create context vector from both LSTM's by concatenating\n",
        "        context_vector = torch.cat((target_h_n, cand_h_n), dim=1)\n",
        "\n",
        "        fc1_o = self.fc1(context_vector)\n",
        "        fc2_o = self.fc2(fc1_o)\n",
        "        logits = self.fc3(fc2_o).squeeze()\n",
        "\n",
        "        return logits  # Removed softmax, logits will be passed to BCEWithLogitsLoss\n"
      ],
      "metadata": {
        "id": "PnADADaGsy0K"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def prepare_batch(batch, sequencer):\n",
        "    target_utts, candidate_utts, triggers = zip(*batch)\n",
        "    input_target_tensor, target_lengths = sequencer.create_padded_tensor(target_utts)\n",
        "    input_candidate_tensor, cand_lengths = sequencer.create_padded_tensor(candidate_utts)\n",
        "    return (input_target_tensor, target_lengths, input_candidate_tensor, cand_lengths, triggers)"
      ],
      "metadata": {
        "id": "LsRssDY9kdsr"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Parameters\n",
        "embedding_dim = 100\n",
        "batch_size = 32\n",
        "hidden_size = 512\n",
        "\n",
        "learning_rate = 0.00001\n",
        "loss_function = nn.BCEWithLogitsLoss()  # Consider using pos weighting\n",
        "\n",
        "train_utts = [utt for sublist in train_df['utterances'] for utt in sublist]\n",
        "sequencer = PreTrainedSequencer(train_utts, word2vec_weights, embedding_dim)\n",
        "\n",
        "model = DualLSTM(\n",
        "    sequencer.pad_index,\n",
        "    embedding_dim=embedding_dim,\n",
        "    batch_size=batch_size,\n",
        "    pre_trained_embeddings = sequencer.pre_trained_embeddings,\n",
        "    hidden_size=hidden_size\n",
        ")\n",
        "#pytorch_total_params = sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
        "#print(pytorch_total_params)  # ~3,000,000 right now\n",
        "\n",
        "optimizer = torch.optim.Adam(model.parameters(), lr=learning_rate)\n",
        "\n",
        "train_dataset = MELDDataset(train_df, sequencer)\n",
        "train_loader = torch.utils.data.DataLoader(train_dataset, batch_size=batch_size, collate_fn=lambda batch: prepare_batch(batch, sequencer))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 390
        },
        "id": "d0OiYYfomM29",
        "outputId": "6dfd8c42-4784-49e4-b5c7-e5237770af7d"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "error",
          "ename": "TypeError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-30-b46e3e54d537>\u001b[0m in \u001b[0;36m<cell line: 7>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[0mlearning_rate\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m0.00001\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 7\u001b[0;31m \u001b[0mloss_function\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mBCEWithLogitsLoss\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpos_weight\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m5.0\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# Weight found from data analysis\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      8\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      9\u001b[0m \u001b[0mtrain_utts\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mutt\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0msublist\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mtrain_df\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'utterances'\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mutt\u001b[0m \u001b[0;32min\u001b[0m \u001b[0msublist\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/modules/loss.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, weight, size_average, reduce, reduction, pos_weight)\u001b[0m\n\u001b[1;32m    718\u001b[0m         \u001b[0msuper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__init__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msize_average\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreduce\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreduction\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    719\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mregister_buffer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'weight'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mweight\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 720\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mregister_buffer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'pos_weight'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpos_weight\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    721\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mweight\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mOptional\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mTensor\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    722\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpos_weight\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mOptional\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mTensor\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36mregister_buffer\u001b[0;34m(self, name, tensor, persistent)\u001b[0m\n\u001b[1;32m    538\u001b[0m             \u001b[0;32mraise\u001b[0m \u001b[0mKeyError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf\"attribute '{name}' already exists\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    539\u001b[0m         \u001b[0;32melif\u001b[0m \u001b[0mtensor\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtensor\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTensor\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 540\u001b[0;31m             raise TypeError(f\"cannot assign '{torch.typename(tensor)}' object to buffer '{name}' \"\n\u001b[0m\u001b[1;32m    541\u001b[0m                             \u001b[0;34m\"(torch Tensor or None required)\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    542\u001b[0m                             )\n",
            "\u001b[0;31mTypeError\u001b[0m: cannot assign 'float' object to buffer 'pos_weight' (torch Tensor or None required)"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def train(model, optimizer, loss_function, loader):\n",
        "    model.train()  # Set the model to training mode\n",
        "    running_loss_history = []\n",
        "    running_loss = 0.\n",
        "    print_step = 25\n",
        "\n",
        "    for i, batch in enumerate(loader):\n",
        "        targets, target_l, candidates, cand_l, triggers = batch\n",
        "        batch_size = targets.shape[0]\n",
        "\n",
        "        if i % print_step == 0:\n",
        "            print(f\"    {i} / {len(loader)}\")\n",
        "\n",
        "        optimizer.zero_grad()  # Zero the gradients\n",
        "\n",
        "        logits = model(targets, target_l, candidates, cand_l)\n",
        "\n",
        "        # Convert triggers to a tensor of dtype float32, if not already\n",
        "        triggers = torch.tensor(triggers, dtype=torch.float32)\n",
        "\n",
        "        # Compute the loss\n",
        "        loss = loss_function(logits, triggers)\n",
        "        running_loss += (loss.item() - running_loss) / (i + 1)\n",
        "        running_loss_history.append(running_loss)\n",
        "\n",
        "        if i % print_step == 0:\n",
        "            print(f'    running loss: {running_loss}')\n",
        "\n",
        "        # Backward pass\n",
        "        loss.backward()\n",
        "\n",
        "        # Gradient Clipping\n",
        "        nn.utils.clip_grad_norm_(model.parameters(), 3.0)\n",
        "\n",
        "        # Optimization step\n",
        "        optimizer.step()\n",
        "\n",
        "    mean_running_loss = np.mean(running_loss_history)\n",
        "    return mean_running_loss\n"
      ],
      "metadata": {
        "id": "9jKeanXTlPZ2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def run_training(model, optimizer, loss_function, train_loader, valid_loader=None, n_epochs=10):\n",
        "    train_running_losses = []\n",
        "\n",
        "    for i in range(n_epochs):\n",
        "        print(f\"EPOCH {i}\")\n",
        "        mean_running_loss = train(model, optimizer, loss_function, train_loader)\n",
        "\n",
        "        # Append the mean running loss for the epoch\n",
        "        train_running_losses.append(mean_running_loss)\n",
        "\n",
        "    return train_running_losses\n"
      ],
      "metadata": {
        "id": "_mRMyWg11dtm"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "run_training(model, optimizer, loss_function, train_loader, n_epochs=1)"
      ],
      "metadata": {
        "id": "qt2E7LdI52qs",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "61f3a39a-3479-4514-bca3-587ee1703e9a"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "EPOCH 0\n",
            "    0 / 871\n",
            "    running loss: 0.6515674591064453\n",
            "    25 / 871\n",
            "    running loss: 0.6343209239152762\n",
            "    50 / 871\n",
            "    running loss: 0.6256203838423188\n",
            "    75 / 871\n",
            "    running loss: 0.6183358470076008\n",
            "    100 / 871\n",
            "    running loss: 0.6112603846162851\n",
            "    125 / 871\n",
            "    running loss: 0.6037156581878661\n",
            "    150 / 871\n",
            "    running loss: 0.5926535774935158\n",
            "    175 / 871\n",
            "    running loss: 0.5815709521147335\n",
            "    200 / 871\n",
            "    running loss: 0.5700720364774635\n",
            "    225 / 871\n",
            "    running loss: 0.557276178909614\n",
            "    250 / 871\n",
            "    running loss: 0.5475377846524059\n",
            "    275 / 871\n",
            "    running loss: 0.5397769117700878\n",
            "    300 / 871\n",
            "    running loss: 0.5306451561245014\n",
            "    325 / 871\n",
            "    running loss: 0.5270089348035355\n",
            "    350 / 871\n",
            "    running loss: 0.5240806364945196\n",
            "    375 / 871\n",
            "    running loss: 0.517772857179033\n",
            "    400 / 871\n",
            "    running loss: 0.5130296660926279\n",
            "    425 / 871\n",
            "    running loss: 0.5094585521540173\n",
            "    450 / 871\n",
            "    running loss: 0.5053077166334223\n",
            "    475 / 871\n",
            "    running loss: 0.50239642421488\n",
            "    500 / 871\n",
            "    running loss: 0.49911016886105747\n",
            "    525 / 871\n",
            "    running loss: 0.4959325216455601\n",
            "    550 / 871\n",
            "    running loss: 0.4941909577149876\n",
            "    575 / 871\n",
            "    running loss: 0.4913735687732694\n",
            "    600 / 871\n",
            "    running loss: 0.491106539815912\n",
            "    625 / 871\n",
            "    running loss: 0.4897536741087609\n",
            "    650 / 871\n",
            "    running loss: 0.4892388346008439\n",
            "    675 / 871\n",
            "    running loss: 0.48841269596441217\n",
            "    700 / 871\n",
            "    running loss: 0.48698884809918447\n",
            "    725 / 871\n",
            "    running loss: 0.48542536265593883\n",
            "    750 / 871\n",
            "    running loss: 0.4844082358991413\n",
            "    775 / 871\n",
            "    running loss: 0.4836400995011792\n",
            "    800 / 871\n",
            "    running loss: 0.48254578531755743\n",
            "    825 / 871\n",
            "    running loss: 0.4811557672430754\n",
            "    850 / 871\n",
            "    running loss: 0.4794630437704142\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[0.5287331507126807]"
            ]
          },
          "metadata": {},
          "execution_count": 22
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def evaluate(model, loss_function, loader):\n",
        "    model.eval()\n",
        "    print_step = 25\n",
        "\n",
        "    accuracy_list = []\n",
        "    for i, batch in enumerate(loader):\n",
        "        targets, target_l, candidates, cand_l, triggers = batch\n",
        "        batch_size = targets.shape[0]\n",
        "\n",
        "        logits = model(targets, target_l, candidates, cand_l)\n",
        "\n",
        "        # Convert triggers to a tensor of dtype float32, if not already\n",
        "        #triggers = torch.tensor(triggers, dtype=torch.float32)\n",
        "\n",
        "        thresh = 0.30  # this is dumb isn't it\n",
        "        sig_layer = torch.nn.Sigmoid()\n",
        "        scores = sig_layer(logits)\n",
        "\n",
        "        pred = [1 if score > thresh else 0 for score in scores]\n",
        "        #print(pred)\n",
        "\n",
        "        accuracy_list.append(accuracy_score(triggers, pred))\n",
        "        if i % print_step == 0:\n",
        "            print(pred)\n",
        "            print(f'current accuracy: {sum(accuracy_list) / i}')\n",
        "\n",
        "    avg_accuracy = sum(accuracy_list) / len(accuracy_list)\n",
        "    print(avg_accuracy)\n",
        "\n",
        "evaluate(model, loss_function, train_loader)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 598
        },
        "id": "UklTFeX0MwHF",
        "outputId": "3939ba16-6687-48ad-a999-b577a4f780a2"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
            "current accuracy: inf\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<ipython-input-49-e1845a5ccab1>:25: RuntimeWarning: divide by zero encountered in double_scalars\n",
            "  print(f'current accuracy: {sum(accuracy_list) / i}')\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
            "current accuracy: 0.88\n",
            "[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
            "current accuracy: 0.87125\n",
            "[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
            "current accuracy: 0.86375\n",
            "[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
            "current accuracy: 0.8575\n",
            "[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
            "current accuracy: 0.8515\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "KeyboardInterrupt",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-49-e1845a5ccab1>\u001b[0m in \u001b[0;36m<cell line: 30>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     28\u001b[0m     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mavg_accuracy\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     29\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 30\u001b[0;31m \u001b[0mevaluate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mloss_function\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrain_loader\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m<ipython-input-49-e1845a5ccab1>\u001b[0m in \u001b[0;36mevaluate\u001b[0;34m(model, loss_function, loader)\u001b[0m\n\u001b[1;32m      8\u001b[0m         \u001b[0mbatch_size\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtargets\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      9\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 10\u001b[0;31m         \u001b[0mlogits\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtargets\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtarget_l\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcandidates\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcand_l\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     11\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     12\u001b[0m         \u001b[0;31m# Convert triggers to a tensor of dtype float32, if not already\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1516\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_compiled_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# type: ignore[misc]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1517\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1518\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1519\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1520\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1525\u001b[0m                 \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_pre_hooks\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_hooks\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1526\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1527\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1528\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1529\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-8-e01803769af1>\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, targets, target_l, candidates, cand_l)\u001b[0m\n\u001b[1;32m     51\u001b[0m         \u001b[0mpacked_cand_i\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mutils\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrnn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpack_padded_sequence\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0membed_cand\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcand_l\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch_first\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0menforce_sorted\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     52\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 53\u001b[0;31m         \u001b[0mo\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mtarget_h_n\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mh_c\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtarget_lstm\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpacked_target_i\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     54\u001b[0m         \u001b[0mo\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mcand_h_n\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mh_c\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcand_lstm\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpacked_cand_i\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     55\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1516\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_compiled_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# type: ignore[misc]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1517\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1518\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1519\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1520\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1525\u001b[0m                 \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_pre_hooks\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_hooks\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1526\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1527\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1528\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1529\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/modules/rnn.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, input, hx)\u001b[0m\n\u001b[1;32m    880\u001b[0m                               self.dropout, self.training, self.bidirectional, self.batch_first)\n\u001b[1;32m    881\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 882\u001b[0;31m             result = _VF.lstm(input, batch_sizes, hx, self._flat_weights, self.bias,\n\u001b[0m\u001b[1;32m    883\u001b[0m                               self.num_layers, self.dropout, self.training, self.bidirectional)\n\u001b[1;32m    884\u001b[0m         \u001b[0moutput\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mresult\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "V7EmQJ09OTH9"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}