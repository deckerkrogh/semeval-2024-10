{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "1502448d-5b30-4d0b-9f40-a2c97364d3e2",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\thsun\\anaconda3\\envs\\ml\\Lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "import random\n",
    "import torch\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import torch.nn as nn\n",
    "\n",
    "from torch.utils.data import DataLoader, RandomSampler, SequentialSampler\n",
    "from transformers import BertTokenizer, BertForSequenceClassification, AdamW, BertConfig, get_linear_schedule_with_warmup\n",
    "from torch.utils.data import TensorDataset, random_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "afbbcd3a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using GPU: NVIDIA GeForce GTX 1070 Ti\n"
     ]
    }
   ],
   "source": [
    "# Set to GPU if available.\n",
    "if torch.cuda.is_available():    \n",
    "    device = torch.device(\"cuda\")\n",
    "    print('Using GPU:', torch.cuda.get_device_name(0))\n",
    "else:\n",
    "    device = torch.device(\"cpu\")\n",
    "    print('No GPU available, using CPU instead.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "1b2695f0-136c-4bad-aae2-2b62f5f08c7c",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\thsun\\anaconda3\\envs\\ml\\Lib\\site-packages\\transformers\\tokenization_utils_base.py:2418: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "# Load the dataset.\n",
    "df = pd.read_json(\"Task 1-20231125T063955Z-001\\Task 1\\MaSaC_train_erc.json\")\n",
    "\n",
    "# Load the bert tokenizer.\n",
    "tokenizer = BertTokenizer.from_pretrained('bert-base-multilingual-uncased', do_lower_case=True)\n",
    "\n",
    "# Get the input ids (bert embedding index) and attention masks (mask of whether a token is padding).\n",
    "# Note that this makes all utterances seperate examples, as bert does not support paragraphs (<SEP> only support a sentence pair).\n",
    "X_input_ids = []\n",
    "X_attention_mask = []\n",
    "\n",
    "for utterances in df[\"utterances\"]:\n",
    "    for utterance in utterances:\n",
    "        encoded_dict = tokenizer.encode_plus(utterance, add_special_tokens = True, max_length = 160, pad_to_max_length = True,\n",
    "                                             return_attention_mask = True, return_tensors = 'pt', truncation = True)\n",
    "        X_input_ids.append(encoded_dict['input_ids'])\n",
    "        X_attention_mask.append(encoded_dict['attention_mask'])\n",
    "\n",
    "# Convert emotions to labels.\n",
    "label_to_index = {'contempt':0, 'anger':1, 'surprise':2, 'fear':3, 'disgust':4, 'sadness':5, 'joy':6, 'neutral':7}\n",
    "Y = []\n",
    "for emotions in df[\"emotions\"]: \n",
    "    Y.extend([label_to_index[emotion] for emotion in emotions])\n",
    "\n",
    "X_input_ids = torch.cat(X_input_ids, dim=0)\n",
    "X_attention_mask = torch.cat(X_attention_mask, dim=0)\n",
    "Y = torch.tensor(Y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "74ab395c-c182-4a36-a6ea-a931aeeb2b3f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prepare the dataset and data loaders.\n",
    "dataset = TensorDataset(X_input_ids, X_attention_mask, Y)\n",
    "\n",
    "train_size = int(0.9 * len(dataset))\n",
    "val_size = len(dataset) - train_size\n",
    "batch_size = 32\n",
    "\n",
    "train_dataset, val_dataset = random_split(dataset, [train_size, val_size])\n",
    "train_dataloader = DataLoader(train_dataset, sampler = RandomSampler(train_dataset), batch_size = batch_size)\n",
    "validation_dataloader = DataLoader(val_dataset, sampler = SequentialSampler(val_dataset), batch_size = batch_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "104098b3-a68a-4491-825a-f96401790fd6",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at bert-base-multilingual-uncased and are newly initialized: ['classifier.weight', 'classifier.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "BertForSequenceClassification(\n",
       "  (bert): BertModel(\n",
       "    (embeddings): BertEmbeddings(\n",
       "      (word_embeddings): Embedding(105879, 768, padding_idx=0)\n",
       "      (position_embeddings): Embedding(512, 768)\n",
       "      (token_type_embeddings): Embedding(2, 768)\n",
       "      (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "      (dropout): Dropout(p=0.1, inplace=False)\n",
       "    )\n",
       "    (encoder): BertEncoder(\n",
       "      (layer): ModuleList(\n",
       "        (0-11): 12 x BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            (intermediate_act_fn): GELUActivation()\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (pooler): BertPooler(\n",
       "      (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "      (activation): Tanh()\n",
       "    )\n",
       "  )\n",
       "  (dropout): Dropout(p=0.1, inplace=False)\n",
       "  (classifier): Linear(in_features=768, out_features=8, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Prepare the model. None of pretrained bert used hinglish, this is the closest.\n",
    "model = BertForSequenceClassification.from_pretrained(\n",
    "    \"bert-base-multilingual-uncased\", num_labels = 8, output_attentions = False, output_hidden_states = False,\n",
    ")\n",
    "model.cuda()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "03dc1cfd-6dc1-49ca-a37e-13dee4818a11",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\thsun\\anaconda3\\envs\\ml\\Lib\\site-packages\\transformers\\optimization.py:411: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "# Prepare the optimizer and learning rate scheduler.\n",
    "epochs = 4\n",
    "total_steps = len(train_dataloader) * epochs\n",
    "optimizer = AdamW(model.parameters(), lr = 2e-5, eps = 1e-8)\n",
    "scheduler = get_linear_schedule_with_warmup(optimizer, num_warmup_steps = 0, num_training_steps = total_steps)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "e4364d1d-e5ca-4f42-a667-732f99afbf4d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def flat_accuracy(preds, labels):\n",
    "    pred_flat = np.argmax(preds, axis=1).flatten()\n",
    "    labels_flat = labels.flatten()\n",
    "    return np.sum(pred_flat == labels_flat) / len(labels_flat)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "284583ff-599c-4099-b0ac-dc2afd95ee00",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "======== Epoch 1 / 4 ========\n",
      "Running Training...\n",
      "  Batch    50  of    240.\n",
      "  Batch   100  of    240.\n",
      "  Batch   150  of    240.\n",
      "  Batch   200  of    240.\n",
      "\n",
      "  Average training loss: 1.60\n",
      "\n",
      "Running Validation...\n",
      "  Accuracy: 0.46\n",
      "  Validation Loss: 1.56\n",
      "\n",
      "======== Epoch 2 / 4 ========\n",
      "Running Training...\n",
      "  Batch    50  of    240.\n",
      "  Batch   100  of    240.\n",
      "  Batch   150  of    240.\n",
      "  Batch   200  of    240.\n",
      "\n",
      "  Average training loss: 1.49\n",
      "\n",
      "Running Validation...\n",
      "  Accuracy: 0.49\n",
      "  Validation Loss: 1.49\n",
      "\n",
      "======== Epoch 3 / 4 ========\n",
      "Running Training...\n",
      "  Batch    50  of    240.\n",
      "  Batch   100  of    240.\n",
      "  Batch   150  of    240.\n",
      "  Batch   200  of    240.\n",
      "\n",
      "  Average training loss: 1.37\n",
      "\n",
      "Running Validation...\n",
      "  Accuracy: 0.50\n",
      "  Validation Loss: 1.48\n",
      "\n",
      "======== Epoch 4 / 4 ========\n",
      "Running Training...\n",
      "  Batch    50  of    240.\n",
      "  Batch   100  of    240.\n",
      "  Batch   150  of    240.\n",
      "  Batch   200  of    240.\n",
      "\n",
      "  Average training loss: 1.26\n",
      "\n",
      "Running Validation...\n",
      "  Accuracy: 0.49\n",
      "  Validation Loss: 1.48\n",
      "\n",
      "Training complete!\n"
     ]
    }
   ],
   "source": [
    "seed_val = 10086\n",
    "random.seed(seed_val)\n",
    "np.random.seed(seed_val)\n",
    "torch.manual_seed(seed_val)\n",
    "torch.cuda.manual_seed_all(seed_val)\n",
    "\n",
    "# Training loop.\n",
    "for epoch_i in range(0, epochs):\n",
    "    print()\n",
    "    print('======== Epoch {:} / {:} ========'.format(epoch_i + 1, epochs))\n",
    "    print('Running Training...')\n",
    "\n",
    "    total_train_loss = 0\n",
    "    model.train()\n",
    "\n",
    "    for step, batch in enumerate(train_dataloader):\n",
    "        if step % 50 == 0 and not step == 0:\n",
    "            print('  Batch {:>5,}  of  {:>5,}.'.format(step, len(train_dataloader)))\n",
    "        \n",
    "        b_input_ids = batch[0].to(device)\n",
    "        b_input_mask = batch[1].to(device)\n",
    "        b_labels = batch[2].to(device)\n",
    "        model.zero_grad()\n",
    "\n",
    "        result = model(b_input_ids, token_type_ids=None, attention_mask=b_input_mask, labels=b_labels, return_dict=True)\n",
    "        loss = result.loss\n",
    "        logits = result.logits\n",
    "        total_train_loss += loss.item()\n",
    "\n",
    "        loss.backward()\n",
    "        torch.nn.utils.clip_grad_norm_(model.parameters(), 1.0)\n",
    "        optimizer.step()\n",
    "        scheduler.step()\n",
    "\n",
    "    avg_train_loss = total_train_loss / len(train_dataloader)\n",
    "    print()\n",
    "    print(\"  Average training loss: {0:.2f}\".format(avg_train_loss))\n",
    "\n",
    "    # After each epoch, run validation once.\n",
    "    print()\n",
    "    print(\"Running Validation...\")\n",
    "\n",
    "    model.eval()\n",
    "\n",
    "    total_eval_accuracy = 0\n",
    "    total_eval_loss = 0\n",
    "    nb_eval_steps = 0\n",
    "\n",
    "    val_dataloader = validation_dataloader\n",
    "    # Change this to True to evaluate using training set.\n",
    "    validate_on_training_set = False\n",
    "    if validate_on_training_set:\n",
    "        val_dataloader = train_dataloader\n",
    "\n",
    "    for batch in val_dataloader:\n",
    "        b_input_ids = batch[0].to(device)\n",
    "        b_input_mask = batch[1].to(device)\n",
    "        b_labels = batch[2].to(device)\n",
    "\n",
    "        with torch.no_grad():\n",
    "            result = model(b_input_ids, token_type_ids=None, attention_mask=b_input_mask, labels=b_labels, return_dict=True)\n",
    "\n",
    "        loss = result.loss\n",
    "        logits = result.logits\n",
    "        total_eval_loss += loss.item()\n",
    "\n",
    "        logits = logits.detach().cpu().numpy()\n",
    "        label_ids = b_labels.to('cpu').numpy()\n",
    "\n",
    "        total_eval_accuracy += flat_accuracy(logits, label_ids)\n",
    "\n",
    "    avg_val_accuracy = total_eval_accuracy / len(val_dataloader)\n",
    "    print(\"  Accuracy: {0:.2f}\".format(avg_val_accuracy))\n",
    "\n",
    "    avg_val_loss = total_eval_loss / len(val_dataloader)\n",
    "    print(\"  Validation Loss: {0:.2f}\".format(avg_val_loss))\n",
    "\n",
    "print()\n",
    "print(\"Training complete!\")\n",
    "\n",
    "# Using dev set, accuracy is ~49%, which is similar to predict everything as neutral.\n",
    "# Using training set, accuracy is ~62%, which means it is already overfitting and remembering examples."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "977b2dc2-2e1d-4afd-886d-8c3ddfbf37ac",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "af1b6565-f5c5-4490-92f2-b9c6c18cd58e",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
