{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "1502448d-5b30-4d0b-9f40-a2c97364d3e2",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "E:\\Users\\thsun\\anaconda3\\envs\\pytorch\\Lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "import random\n",
    "import torch\n",
    "import os\n",
    "import csv\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import torch.nn as nn\n",
    "\n",
    "from torch.utils.data import DataLoader, RandomSampler, SequentialSampler\n",
    "from transformers import BertTokenizer, BertForSequenceClassification, AdamW, get_linear_schedule_with_warmup\n",
    "from torch.utils.data import TensorDataset, random_split\n",
    "from sklearn.metrics import classification_report"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "afbbcd3a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using GPU: NVIDIA GeForce GTX 1070 Ti\n"
     ]
    }
   ],
   "source": [
    "# Set to GPU if available.\n",
    "if torch.cuda.is_available():    \n",
    "    device = torch.device(\"cuda\")\n",
    "    print('Using GPU:', torch.cuda.get_device_name(0))\n",
    "else:\n",
    "    device = torch.device(\"cpu\")\n",
    "    print('No GPU available, using CPU instead.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "1b2695f0-136c-4bad-aae2-2b62f5f08c7c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the dataset.\n",
    "def load_dataset(fname):\n",
    "    df = pd.read_json(fname)\n",
    "    \n",
    "    # Load the bert tokenizer.\n",
    "    tokenizer = BertTokenizer.from_pretrained('bert-base-uncased', do_lower_case=True)\n",
    "    \n",
    "    # Get the input ids (bert embedding index) and attention masks (mask of whether a token is padding).\n",
    "    # Note that this makes all utterances seperate examples, as bert does not support paragraphs (<SEP> only support a sentence pair).\n",
    "    X_input_ids = []\n",
    "    X_attention_mask = []\n",
    "    \n",
    "    for utterances in df[\"utterances\"]:\n",
    "        for utterance in utterances:\n",
    "            encoded_dict = tokenizer.encode_plus(utterance, add_special_tokens = True, max_length = 160, pad_to_max_length = True,\n",
    "                                                 return_attention_mask = True, return_tensors = 'pt', truncation = True)\n",
    "            X_input_ids.append(encoded_dict['input_ids'])\n",
    "            X_attention_mask.append(encoded_dict['attention_mask'])\n",
    "    \n",
    "    # Convert emotions to labels.\n",
    "    label_to_index = {'contempt':0, 'anger':1, 'surprise':2, 'fear':3, 'disgust':4, 'sadness':5, 'joy':6, 'neutral':7}\n",
    "    Y = []\n",
    "    for emotions in df[\"emotions\"]: \n",
    "        Y.extend([label_to_index[emotion] for emotion in emotions])\n",
    "    \n",
    "    X_input_ids = torch.cat(X_input_ids, dim=0)\n",
    "    X_attention_mask = torch.cat(X_attention_mask, dim=0)\n",
    "    Y = torch.tensor(Y)\n",
    "    return TensorDataset(X_input_ids, X_attention_mask, Y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "74ab395c-c182-4a36-a6ea-a931aeeb2b3f",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "E:\\Users\\thsun\\anaconda3\\envs\\pytorch\\Lib\\site-packages\\transformers\\tokenization_utils_base.py:2418: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
      "  warnings.warn(\n",
      "E:\\Users\\thsun\\anaconda3\\envs\\pytorch\\Lib\\site-packages\\transformers\\tokenization_utils_base.py:2418: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "# Prepare the dataset and data loaders.\n",
    "# dataset = TensorDataset(X_input_ids, X_attention_mask, Y)\n",
    "\n",
    "# train_size = int(0.9 * len(dataset))\n",
    "# val_size = len(dataset) - train_size\n",
    "# batch_size = 32\n",
    "\n",
    "# train_dataset, val_dataset = random_split(dataset, [train_size, val_size])\n",
    "\n",
    "batch_size = 32\n",
    "train_dataset = load_dataset(\"Task 3-20231125T064001Z-001\\Task 3\\MELD_train_efr.json\")\n",
    "val_dataset = load_dataset(\"MELD_val_efr.json\")\n",
    "\n",
    "train_dataloader = DataLoader(train_dataset, sampler = RandomSampler(train_dataset), batch_size = batch_size)\n",
    "validation_dataloader = DataLoader(val_dataset, sampler = SequentialSampler(val_dataset), batch_size = batch_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "104098b3-a68a-4491-825a-f96401790fd6",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at bert-base-uncased and are newly initialized: ['classifier.weight', 'classifier.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "BertForSequenceClassification(\n",
       "  (bert): BertModel(\n",
       "    (embeddings): BertEmbeddings(\n",
       "      (word_embeddings): Embedding(30522, 768, padding_idx=0)\n",
       "      (position_embeddings): Embedding(512, 768)\n",
       "      (token_type_embeddings): Embedding(2, 768)\n",
       "      (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "      (dropout): Dropout(p=0.1, inplace=False)\n",
       "    )\n",
       "    (encoder): BertEncoder(\n",
       "      (layer): ModuleList(\n",
       "        (0-11): 12 x BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            (intermediate_act_fn): GELUActivation()\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (pooler): BertPooler(\n",
       "      (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "      (activation): Tanh()\n",
       "    )\n",
       "  )\n",
       "  (dropout): Dropout(p=0.1, inplace=False)\n",
       "  (classifier): Linear(in_features=768, out_features=8, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Prepare the model.\n",
    "model = BertForSequenceClassification.from_pretrained(\n",
    "    \"bert-base-uncased\", num_labels = 8, output_attentions = False, output_hidden_states = False,\n",
    ")\n",
    "model.cuda()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "03dc1cfd-6dc1-49ca-a37e-13dee4818a11",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "E:\\Users\\thsun\\anaconda3\\envs\\pytorch\\Lib\\site-packages\\transformers\\optimization.py:411: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "# Prepare the optimizer and learning rate scheduler.\n",
    "epochs = 4\n",
    "total_steps = len(train_dataloader) * epochs\n",
    "optimizer = AdamW(model.parameters(), lr = 5e-5, eps = 1e-8)\n",
    "scheduler = get_linear_schedule_with_warmup(optimizer, num_warmup_steps = 0, num_training_steps = total_steps)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "e4364d1d-e5ca-4f42-a667-732f99afbf4d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluation.\n",
    "def evaluation(model):\n",
    "    print(\"Running Validation...\")\n",
    "    index_to_label = {0:'contempt', 1:'anger', 2:'surprise', 3:'fear', 4:'disgust', 5:'sadness', 6:'joy', 7:'neutral'}\n",
    "    model.eval()\n",
    "    total_eval_loss = 0\n",
    "    predictions = []\n",
    "    labels = []\n",
    "    for batch in validation_dataloader:\n",
    "        b_input_ids = batch[0].to(device)\n",
    "        b_input_mask = batch[1].to(device)\n",
    "        b_labels = batch[2].to(device)\n",
    "    \n",
    "        with torch.no_grad():\n",
    "            result = model(b_input_ids, token_type_ids=None, attention_mask=b_input_mask, labels=b_labels, return_dict=True)\n",
    "    \n",
    "        logits = result.logits\n",
    "        total_eval_loss += loss.item()\n",
    "    \n",
    "        logits = logits.detach().cpu().numpy()\n",
    "        label_ids = b_labels.to('cpu').numpy()\n",
    "    \n",
    "        predictions.extend([index_to_label[i] for i in np.argmax(logits, axis=1).flatten()])\n",
    "        labels.extend([index_to_label[i] for i in label_ids.flatten()])\n",
    "\n",
    "    avg_val_loss = total_eval_loss / len(validation_dataloader)\n",
    "    print(\"Validation Loss: {0:.2f}\".format(avg_val_loss))\n",
    "    print(classification_report(labels, predictions))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "284583ff-599c-4099-b0ac-dc2afd95ee00",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "======== Epoch 1 / 4 ========\n",
      "Running Training...\n",
      "  Batch   100  of  1,094.\n",
      "  Batch   200  of  1,094.\n",
      "  Batch   300  of  1,094.\n",
      "  Batch   400  of  1,094.\n",
      "  Batch   500  of  1,094.\n",
      "  Batch   600  of  1,094.\n",
      "  Batch   700  of  1,094.\n",
      "  Batch   800  of  1,094.\n",
      "  Batch   900  of  1,094.\n",
      "  Batch 1,000  of  1,094.\n",
      "\n",
      "  Average training loss: 0.73\n",
      "\n",
      "Running Validation...\n",
      "Validation Loss: 0.47\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "       anger       0.43      0.33      0.37       482\n",
      "     disgust       0.35      0.48      0.41        64\n",
      "        fear       0.30      0.28      0.29       156\n",
      "         joy       0.55      0.59      0.57       597\n",
      "     neutral       0.67      0.81      0.73      1360\n",
      "     sadness       0.36      0.28      0.31       343\n",
      "    surprise       0.58      0.41      0.48       520\n",
      "\n",
      "    accuracy                           0.57      3522\n",
      "   macro avg       0.46      0.45      0.45      3522\n",
      "weighted avg       0.55      0.57      0.55      3522\n",
      "\n",
      "\n",
      "======== Epoch 2 / 4 ========\n",
      "Running Training...\n",
      "  Batch   100  of  1,094.\n",
      "  Batch   200  of  1,094.\n",
      "  Batch   300  of  1,094.\n",
      "  Batch   400  of  1,094.\n",
      "  Batch   500  of  1,094.\n",
      "  Batch   600  of  1,094.\n",
      "  Batch   700  of  1,094.\n",
      "  Batch   800  of  1,094.\n",
      "  Batch   900  of  1,094.\n",
      "  Batch 1,000  of  1,094.\n",
      "\n",
      "  Average training loss: 0.23\n",
      "\n",
      "Running Validation...\n",
      "Validation Loss: 0.11\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "       anger       0.46      0.26      0.33       482\n",
      "     disgust       0.49      0.45      0.47        64\n",
      "        fear       0.52      0.28      0.36       156\n",
      "         joy       0.55      0.48      0.51       597\n",
      "     neutral       0.61      0.85      0.71      1360\n",
      "     sadness       0.30      0.17      0.22       343\n",
      "    surprise       0.50      0.48      0.49       520\n",
      "\n",
      "    accuracy                           0.55      3522\n",
      "   macro avg       0.49      0.42      0.44      3522\n",
      "weighted avg       0.53      0.55      0.52      3522\n",
      "\n",
      "\n",
      "======== Epoch 3 / 4 ========\n",
      "Running Training...\n",
      "  Batch   100  of  1,094.\n",
      "  Batch   200  of  1,094.\n",
      "  Batch   300  of  1,094.\n",
      "  Batch   400  of  1,094.\n",
      "  Batch   500  of  1,094.\n",
      "  Batch   600  of  1,094.\n",
      "  Batch   700  of  1,094.\n",
      "  Batch   800  of  1,094.\n",
      "  Batch   900  of  1,094.\n",
      "  Batch 1,000  of  1,094.\n",
      "\n",
      "  Average training loss: 0.13\n",
      "\n",
      "Running Validation...\n",
      "Validation Loss: 0.02\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "       anger       0.46      0.24      0.32       482\n",
      "     disgust       0.41      0.33      0.37        64\n",
      "        fear       0.27      0.19      0.22       156\n",
      "         joy       0.47      0.60      0.53       597\n",
      "     neutral       0.68      0.73      0.70      1360\n",
      "     sadness       0.29      0.26      0.27       343\n",
      "    surprise       0.48      0.55      0.51       520\n",
      "\n",
      "    accuracy                           0.54      3522\n",
      "   macro avg       0.44      0.41      0.42      3522\n",
      "weighted avg       0.52      0.54      0.52      3522\n",
      "\n",
      "\n",
      "======== Epoch 4 / 4 ========\n",
      "Running Training...\n",
      "  Batch   100  of  1,094.\n",
      "  Batch   200  of  1,094.\n",
      "  Batch   300  of  1,094.\n",
      "  Batch   400  of  1,094.\n",
      "  Batch   500  of  1,094.\n",
      "  Batch   600  of  1,094.\n",
      "  Batch   700  of  1,094.\n",
      "  Batch   800  of  1,094.\n",
      "  Batch   900  of  1,094.\n",
      "  Batch 1,000  of  1,094.\n",
      "\n",
      "  Average training loss: 0.09\n",
      "\n",
      "Running Validation...\n",
      "Validation Loss: 0.03\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "       anger       0.44      0.33      0.38       482\n",
      "     disgust       0.33      0.28      0.30        64\n",
      "        fear       0.26      0.28      0.27       156\n",
      "         joy       0.53      0.56      0.54       597\n",
      "     neutral       0.66      0.76      0.71      1360\n",
      "     sadness       0.34      0.23      0.27       343\n",
      "    surprise       0.51      0.51      0.51       520\n",
      "\n",
      "    accuracy                           0.55      3522\n",
      "   macro avg       0.44      0.42      0.43      3522\n",
      "weighted avg       0.53      0.55      0.54      3522\n",
      "\n",
      "\n",
      "Training complete!\n"
     ]
    }
   ],
   "source": [
    "seed_val = 10086\n",
    "random.seed(seed_val)\n",
    "np.random.seed(seed_val)\n",
    "torch.manual_seed(seed_val)\n",
    "torch.cuda.manual_seed_all(seed_val)\n",
    "\n",
    "# Training loop.\n",
    "for epoch_i in range(0, epochs):\n",
    "    print()\n",
    "    print('======== Epoch {:} / {:} ========'.format(epoch_i + 1, epochs))\n",
    "    print('Running Training...')\n",
    "\n",
    "    total_train_loss = 0\n",
    "    model.train()\n",
    "\n",
    "    for step, batch in enumerate(train_dataloader):\n",
    "        if step % 100 == 0 and not step == 0:\n",
    "            print('  Batch {:>5,}  of  {:>5,}.'.format(step, len(train_dataloader)))\n",
    "        \n",
    "        b_input_ids = batch[0].to(device)\n",
    "        b_input_mask = batch[1].to(device)\n",
    "        b_labels = batch[2].to(device)\n",
    "        model.zero_grad()\n",
    "\n",
    "        result = model(b_input_ids, token_type_ids=None, attention_mask=b_input_mask, labels=b_labels, return_dict=True)\n",
    "        loss = result.loss\n",
    "        logits = result.logits\n",
    "        total_train_loss += loss.item()\n",
    "\n",
    "        loss.backward()\n",
    "        torch.nn.utils.clip_grad_norm_(model.parameters(), 1.0)\n",
    "        optimizer.step()\n",
    "        scheduler.step()\n",
    "\n",
    "    avg_train_loss = total_train_loss / len(train_dataloader)\n",
    "    print()\n",
    "    print(\"  Average training loss: {0:.2f}\".format(avg_train_loss))\n",
    "    print()\n",
    "\n",
    "    # After each epoch, run validation once.\n",
    "    evaluation(model)\n",
    "\n",
    "print()\n",
    "print(\"Training complete!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "977b2dc2-2e1d-4afd-886d-8c3ddfbf37ac",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('\\\\model_save\\\\tokenizer_config.json',\n",
       " '\\\\model_save\\\\special_tokens_map.json',\n",
       " '\\\\model_save\\\\vocab.txt',\n",
       " '\\\\model_save\\\\added_tokens.json')"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Save the model.\n",
    "output_dir = '\\model_save'\n",
    "if not os.path.exists(output_dir):\n",
    "    os.makedirs(output_dir)\n",
    "\n",
    "model_to_save = model.module if hasattr(model, 'module') else model\n",
    "model_to_save.save_pretrained(output_dir)\n",
    "tokenizer = BertTokenizer.from_pretrained('bert-base-uncased', do_lower_case=True)\n",
    "tokenizer.save_pretrained(output_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "af1b6565-f5c5-4490-92f2-b9c6c18cd58e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "BertForSequenceClassification(\n",
       "  (bert): BertModel(\n",
       "    (embeddings): BertEmbeddings(\n",
       "      (word_embeddings): Embedding(30522, 768, padding_idx=0)\n",
       "      (position_embeddings): Embedding(512, 768)\n",
       "      (token_type_embeddings): Embedding(2, 768)\n",
       "      (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "      (dropout): Dropout(p=0.1, inplace=False)\n",
       "    )\n",
       "    (encoder): BertEncoder(\n",
       "      (layer): ModuleList(\n",
       "        (0-11): 12 x BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            (intermediate_act_fn): GELUActivation()\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (pooler): BertPooler(\n",
       "      (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "      (activation): Tanh()\n",
       "    )\n",
       "  )\n",
       "  (dropout): Dropout(p=0.1, inplace=False)\n",
       "  (classifier): Linear(in_features=768, out_features=8, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Load the saved model.\n",
    "output_dir = '\\model_save'\n",
    "saved_model = BertForSequenceClassification.from_pretrained(output_dir, output_hidden_states = True)\n",
    "saved_tokenizer = BertTokenizer.from_pretrained(output_dir)\n",
    "saved_model.to(device)\n",
    "# evaluation(saved_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "9826958c-3538-4eb9-b286-23e6a43c9cd6",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "E:\\Users\\thsun\\anaconda3\\envs\\pytorch\\Lib\\site-packages\\transformers\\tokenization_utils_base.py:2418: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processed 100 batches\n",
      "Processed 200 batches\n",
      "Processed 300 batches\n",
      "Processed 400 batches\n",
      "Processed 500 batches\n",
      "Processed 600 batches\n",
      "Processed 700 batches\n",
      "Processed 800 batches\n",
      "Processed 900 batches\n",
      "Processed 1000 batches\n",
      "Processed 1100 batches\n",
      "Processed 1200 batches\n",
      "Processed 1300 batches\n",
      "Processed 1400 batches\n"
     ]
    }
   ],
   "source": [
    "# Extract sentence embeddings.\n",
    "def extract_embedding(model):\n",
    "    test_dataset = load_dataset(\"MELD_test_efr.json\")\n",
    "    test_dataloader = DataLoader(test_dataset, sampler = SequentialSampler(test_dataset), batch_size = batch_size)\n",
    "    model.eval()\n",
    "    cnt = 0\n",
    "    fnames = ['task3_sentence_embedding_train.csv', 'task3_sentence_embedding_val.csv', 'task3_sentence_embedding_test.csv']\n",
    "    idx = 0\n",
    "    for data_loader in [train_dataloader, validation_dataloader, test_dataloader]:\n",
    "        with open(fnames[idx], 'w', newline='') as csvfile:\n",
    "            embedding_writer = csv.writer(csvfile, delimiter=' ')\n",
    "            for batch in data_loader:\n",
    "                b_input_ids = batch[0].to(device)\n",
    "                b_input_mask = batch[1].to(device)\n",
    "                b_labels = batch[2].to(device)\n",
    "                cnt += 1\n",
    "                if cnt % 100 == 0:\n",
    "                    print(\"Processed\", cnt, \"batches\")\n",
    "            \n",
    "                with torch.no_grad():\n",
    "                    result = model(b_input_ids, token_type_ids=None, attention_mask=b_input_mask, labels=b_labels, return_dict=True)\n",
    "                    # Sentence embedding is the states of the first token of the last hidden layer for BertForSequenceClassification.\n",
    "                    embedding_batch = result.hidden_states[-1][:,0,:]\n",
    "                    for embedding in embedding_batch:\n",
    "                        embedding_writer.writerow(embedding.cpu().numpy())\n",
    "        idx += 1\n",
    "\n",
    "extract_embedding(saved_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8a52669e-2eef-4188-a0d6-17c22da26a69",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
